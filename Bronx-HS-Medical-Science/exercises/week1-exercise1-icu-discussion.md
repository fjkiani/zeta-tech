# WEEK 1, EXERCISE 1: The ICU Risk Predictor Discussion
## Bronx HS for Medical Science — "Data to Diagnosis"

**Exercise Type**: Hook / Clinical Case Discussion  
**Duration**: 8 minutes  
**Timing in Lesson**: 0:00–0:08 (Opening)  
**Mode**: Whole class, then think-pair-share  

---

## LEARNING OBJECTIVE

Students will identify what data medical AI systems use and recognize that AI predictions require human clinical judgment.

---

## MATERIALS NEEDED

- [ ] Slide 2: The ICU scenario (projected)
- [ ] Slide 3: Think-pair-share prompt (projected)
- [ ] Optional: Simple ICU monitor visual (heart rate, BP, SpO2, temp display)
- [ ] Whiteboard/markers to capture student responses

---

## TEACHER PREPARATION (Before Class)

1. **Review the scenario**: Familiarize yourself with ICU monitoring basics (heart rate, blood pressure, oxygen saturation, temperature, basic labs). You don't need to be a clinician—just enough to guide the discussion.

2. **Anticipate student responses**:
   - Data AI might use: vitals (HR, BP, temp, O2), lab results (white blood cells, lactate, creatinine), patient history (age, prior conditions)
   - Concerns: false alarms, missed cases, who's responsible if wrong, bias in training data

3. **Set the tone**: This is NOT a trick question. There are no wrong answers. You want students thinking critically and asking "what if?"

---

## STEP-BY-STEP INSTRUCTIONS

### MINUTE 0:00–0:02 — Introduce the Scenario (2 min)

**What you do:**

1. Display **Slide 2** (The ICU scenario).

2. Read the scenario aloud slowly and clearly:

> *"A patient is in the ICU. Their vitals are being monitored—heart rate, blood pressure, temperature, oxygen. Lab results are coming in. In the background, a hospital system is analyzing that data. It's not a doctor—it's an AI. It's been trained on thousands of past ICU patients. It's predicting: 'This patient has a high risk of deteriorating in the next 6 hours.' The nurse gets an alert. The doctor has to decide: act now, or wait? What if the AI is wrong and we overtreat? What if it's right and we miss the window? Who's responsible?"*

3. Pause for 5–10 seconds. Let the stakes sink in.

**What students do:**
- Listen
- Visualize the scenario

**Teacher tip**: Use a calm, serious tone—this is real. Sepsis and patient deterioration are real ICU challenges.

---

### MINUTE 0:02–0:04 — Initial Think Time (2 min)

**What you do:**

1. Display **Slide 3** (Think-Pair-Share prompt):

> *"What data do you think the AI used to make that prediction? What could go wrong if we rely on it without checking?"*

2. Say: *"Take 30 seconds. Think silently: What data did the AI look at? Write down 2–3 things you think it might use."*

3. After 30 seconds: *"Now turn to a partner. Share your ideas. Also discuss: What could go wrong if the doctor just trusts the AI without checking?"*

**What students do:**
- **Individual think** (30 sec): jot down 2–3 data types (e.g., heart rate, labs, age)
- **Pair share** (90 sec): discuss with a partner

**Teacher tip**: Circulate quickly. Listen for good insights to call on during share-out.

---

### MINUTE 0:04–0:07 — Whole-Class Share-Out (3 min)

**What you do:**

1. Call on 2–3 pairs. Ask:
   - *"What data do you think the AI used?"*
   - *"What could go wrong?"*

2. Write responses on the board under two columns:
   - **Data AI Might Use**
   - **Risks / What Could Go Wrong**

3. Expected responses:
   - **Data**: heart rate, blood pressure, temperature, oxygen levels, lab results (white blood cells, infection markers), patient age, medical history
   - **Risks**: false alarm (overtreat healthy patients), missed cases (AI misses someone who's actually at risk), bias (trained on certain populations, not others), who's responsible if wrong, privacy

4. Affirm all responses: *"Great. You're already thinking like clinicians: What's the evidence? What could go wrong?"*

**What students do:**
- Share out their ideas
- Listen to peers
- Add to their own thinking

**Teacher tip**: If students only focus on "data," prompt: *"And what if the AI is wrong? What happens?"* If they only focus on risks, prompt: *"What data would help the AI make that prediction?"*

---

### MINUTE 0:07–0:08 — Transition (1 min)

**What you do:**

Say: *"That system is medical AI. Today we're going to learn what it is, where it's already used, and why we have to verify it—because in medicine, the stakes are life and death."*

Move to Slide 4 (Medical AI in Plain Language) to begin the mini-lesson.

**What students do:**
- Shift from discussion to listening/learning mode

---

## KEY POINTS TO EMPHASIZE

1. **AI uses patient data** (vitals, labs, history) to make predictions.
2. **AI doesn't replace the doctor**—it provides a tool, but the clinician decides.
3. **Verification matters** because wrong predictions can harm patients (false positives = unnecessary treatment; false negatives = missed cases).
4. **Real stakes**: This isn't hypothetical—systems like this are already in use.

---

## COMMON STUDENT RESPONSES & HOW TO HANDLE

| Student Says | How to Respond |
|:-------------|:---------------|
| "The AI uses heart rate and blood pressure" | "Yes! Those are vitals. What else might it look at—lab results? Patient history?" |
| "What if the AI is biased?" | "Excellent question. If it's trained mostly on one population, it might not work as well for others. We'll talk more about bias later this semester." |
| "Who's responsible if the AI is wrong?" | "That's the big question. Right now, the doctor is still responsible—AI is a tool, not the decision-maker." |
| "Can the AI be hacked?" | "Good security concern. Privacy and data security are huge in healthcare. We'll touch on that when we talk about risks." |
| "I don't trust AI" | "That's a valid stance. The question is: under what conditions WOULD you trust it? What would you need to know?" |

---

## FALLBACK STRATEGIES

**If students are silent:**
- Cold-call gently: *"Let's start simple. If you were monitoring a patient, what would you check first? Heart rate? Breathing?"*
- Provide a hint: *"Think about what a nurse checks when they come into the room—vitals, right? What are vitals?"*

**If discussion goes too long:**
- After 2–3 pairs share, say: *"Lots of great ideas. Let's capture these and come back to them as we learn more today."*

**If a student brings up something too advanced** (e.g., specific ML algorithms):
- Affirm: *"Great knowledge! We'll get into how it works later. For now, we're focusing on what it does and why we need to check it."*

---

## ASSESSMENT (Formative)

**Look for:**
- [ ] Can students name 2–3 types of patient data (vitals, labs, history)?
- [ ] Do students recognize at least one risk (false alarm, missed case, bias, responsibility)?
- [ ] Are students asking "what if?" questions (skeptical, evidence-based thinking)?

**If most students can do this** → proceed to mini-lesson.  
**If students are confused** → spend 1–2 extra minutes clarifying: "AI looks at data. It makes predictions. Doctors still decide."

---

## TEACHER REFLECTION (After Class)

- Did the scenario engage students immediately?
- Which student responses were most insightful?
- Did any students seem confused about what AI is? (Note for mini-lesson adjustment.)
- Time check: Did this take 8 minutes, or did you need more/less?

---

*"The best clinical question is: What could go wrong?"*
