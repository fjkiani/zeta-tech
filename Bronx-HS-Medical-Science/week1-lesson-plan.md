# Week 1: AI in Medicine & Science
## Bronx HS for Medical Science — "Data to Diagnosis"

**Duration**: 44 minutes | **Theme**: AI in Healthcare

---

## 1. LEARNING OBJECTIVES

| # | Learning Objective | Bloom's Level | How Assessed |
|---|--------------------|---------------|--------------|
| 1 | Explain medical AI in plain language as pattern-finding in patient data to support clinical decisions | Understand | Exit ticket; mini-lesson check |
| 2 | Identify 2–3 real-world healthcare applications of AI (diagnosis support, risk prediction, imaging, drug discovery) | Remember / Apply | Slide discussion; guided practice handout |
| 3 | Describe Ask → Verify → Refine and why verification is critical in medical contexts (patient safety, evidence-based practice) | Understand | Mini-lesson; exit ticket |
| 4 | Write one clear prompt asking an AI to explain a medical concept, then fact-check the response against a trusted source | Apply / Analyze | Guided practice worksheet; observation |
| 5 | Articulate one risk of AI in healthcare (misdiagnosis, bias, over-reliance, privacy) and one benefit (access, speed, consistency) | Analyze | Debrief share-out; exit ticket |

---

## 2. LESSON PLAN OUTLINE

### 0:00–0:08 — HOOK + CLINICAL CASE INTRO (8 min)

**Scenario: "The ICU Risk Predictor"**

> *"A patient is in the ICU. Their vitals are being monitored—heart rate, blood pressure, temperature, oxygen. Lab results are coming in. In the background, a hospital system is analyzing that data. It’s not a doctor—it’s an AI. It’s been trained on thousands of past ICU patients. It’s predicting: 'This patient has a high risk of deteriorating in the next 6 hours.' The nurse gets an alert. The doctor has to decide: act now, or wait? What if the AI is wrong and we overtreat? What if it’s right and we miss the window? Who’s responsible?"*

**Discussion prompts:**
- What data do you think the AI looked at to make that prediction?
- As the doctor, what would you do with that alert?
- What happens if the AI misses a patient who actually deteriorates?

**Transition:** *"That system is medical AI. Today we’re going to learn what it is, where it’s already used, and why we have to verify it—because in medicine, the stakes are life and death."*

---

### 0:08–0:22 — MINI-LESSON + DEMO (14 min)

**Core content:**

1. **Medical AI in plain language** (3 min)  
   - AI in medicine = **finding patterns in patient data** (vitals, labs, imaging, history) to **predict outcomes** or **suggest diagnoses**  
   - It doesn’t replace the doctor—it supports clinical judgment  
   - One phrase: *"Show me enough past patients, I’ll help predict what might happen next."*

2. **Supervised learning in a medical context** (4 min)  
   - **Examples** = past patients (their data + what actually happened)  
   - **Features** = inputs (symptoms, test results, vitals, age, history)  
   - **Outcomes** = what we’re predicting (diagnosis, risk level, deterioration yes/no)  
   - Connect to what they know: *"Like forming a hypothesis from data—but the machine finds the patterns from thousands of cases."*

3. **Where AI is already used in healthcare** (4 min)  
   - **Radiology:** flagging possible findings on X-rays, CT, MRI  
   - **ICU / monitoring:** sepsis and deterioration prediction (today’s scenario)  
   - **Pathology:** helping analyze slides  
   - **Drug discovery:** screening compounds  
   - **Clinical trials:** matching patients to trials  
   - Point: *"It’s already in hospitals. The question is how we use it safely."*

4. **Ask → Verify → Refine** (3 min)  
   - **Ask:** Clear question to an AI (e.g., “Explain sepsis risk factors”)  
   - **Verify:** Fact-check against trusted sources (CDC, NIH, PubMed, textbook)—*critical in medicine*  
   - **Refine:** If the answer is wrong or incomplete, ask again or rephrase  
   - Emphasize: *"In medicine we don’t act on the first answer. We verify. Patient safety first."*  
   - Connect to clinical workflow: gather data → form hypothesis → test against evidence → revise.

**Demo (if AI available):**  
Ask an AI: *"Explain in one sentence what sepsis is and why early detection matters in the ICU."*  
Then: *"How would we verify that? What source would we check?"* —model the verification step.

---

### 0:22–0:36 — GUIDED PRACTICE (14 min)

**Activity: "Prompt, Explain, Verify"**

Students work in pairs with the **Student Handout** (see Section 5). Tasks:

1. **Prompt writing** (5 min): Write a clear prompt asking an AI to explain one medical concept from today (e.g., “What vital signs are used to predict deterioration in the ICU?” or “What is sepsis and what are early warning signs?”).  
   - If AI is available: run the prompt and read the answer.  
   - If blocked: use teacher’s demo or a provided sample response.

2. **Fact-check** (5 min): Use the handout’s fact-checking checklist. Compare the AI’s answer to one trusted source (CDC, NIH, or textbook). Note: What did it get right? What was incomplete or wrong?

3. **Risks and benefits** (4 min): With a partner, list **one risk** of using AI in healthcare (misdiagnosis, bias, over-reliance, privacy) and **one benefit** (access, speed, consistency). Write 1–2 sentences to share.

**Teacher circulates:**  
Check prompt clarity, encourage use of CDC/NIH for verification, and support the risk/benefit discussion.

---

### 0:36–0:42 — DEBRIEF / SHARE-OUT (6 min)

**Structure:**

1. **Prompt + verification** (3 min): 2 students (or pairs) share their prompt and one thing they verified (or would verify) against a trusted source.

2. **Risks and benefits** (3 min): 2 students share one risk and one benefit.  
   - Reinforce: *"Accuracy alone isn’t enough. We also need to ask: Who does it help? Who might it harm? Is it fair across different populations?"*

**Closure:**  
*"You’re not just future clinicians—you’re future users of clinical AI. Your job is to use it responsibly and verify everything. First, do no harm."*

---

### 0:42–0:44 — EXIT TICKET (2 min)

Students submit via Google Classroom (exact wording in Section 4).

---

## 3. SLIDE DECK OUTLINE (11 slides)

### Slide 1: Title
- **Title:** Data to Diagnosis: AI in Healthcare  
- **Subtitle:** Week 1 — What is Medical AI?  
- **Visual:** Hospital or ICU monitor (non-graphic); or abstract data + stethoscope  
- **Notes:** Welcome. *"Today we start with one question: How is AI already helping—and sometimes risking—patient care?"*

---

### Slide 2: The Scenario
- **Title:** Before We Define Anything… A Scenario  
- **Bullets:**
  - Patient in the ICU; vitals and labs monitored
  - AI analyzes the data (trained on thousands of past patients)
  - Predicts: “High risk of deterioration in next 6 hours”
  - Nurse gets an alert → doctor must decide: act now or wait?
- **Visual:** Simplified ICU monitor or dashboard (heart rate, BP, SpO2, temp); no graphic patient images  
- **Notes:** Read the scenario. *"As the doctor, what would you do with that alert? What if the AI is wrong?"*

---

### Slide 3: Think-Pair-Share (Interactive)
- **Title:** Turn to a Partner  
- **Prompt:** *"What data do you think the AI used to make that prediction? What could go wrong if we rely on it without checking?"*  
- **Visual:** Discussion icon or clinician at monitor  
- **Notes:** 2 min. Call on 2–3 pairs. Draw out: vitals, labs, history—and risks of false alarms or missed cases.

---

### Slide 4: Medical AI in Plain Language
- **Title:** What Is Medical AI?  
- **Bullets:**
  - Finds **patterns** in patient data (vitals, labs, imaging, history)
  - Used to **predict** outcomes or **suggest** diagnoses
  - Supports the clinician—does not replace the doctor
- **Visual:** Simple flow: Patient Data → Patterns → Prediction / Suggestion  
- **Notes:** *"One sentence: Medical AI finds patterns in patient data to help doctors make decisions."*

---

### Slide 5: Features and Outcomes (Medical Context)
- **Title:** What Goes In, What Comes Out  
- **Bullets:**
  - **Features** = inputs (symptoms, test results, vitals, age, history)
  - **Outcomes** = what we predict (diagnosis, risk level, deterioration yes/no)
  - AI learns from past patients: “When I see these features, this outcome often follows”
- **Visual:** Diagram: [Features: vitals, labs, history] → [???] → [Outcome: risk level / diagnosis]  
- **Notes:** *"Same idea as hypothesis from data—but the machine uses many more cases."*

---

### Slide 6: Where AI Is Already Used in Healthcare
- **Title:** It’s Already in the Hospital  
- **Bullets:**
  - Radiology (X-ray, CT, MRI—flagging possible findings)
  - ICU monitoring (sepsis, deterioration prediction)
  - Pathology (analyzing slides)
  - Drug discovery and clinical trials
- **Visual:** Icons for radiology, ICU, lab, pill/bottle  
- **Notes:** *"Which of these have you heard about? Which might you use in your career?"*

---

### Slide 7: Ask → Verify → Refine
- **Title:** Your Workflow When Using AI in Medicine  
- **Bullets:**
  - **Ask:** Write a clear question
  - **Verify:** Fact-check against trusted sources (CDC, NIH, PubMed, textbook)—*patient safety first*
  - **Refine:** If wrong or incomplete, ask again or rephrase
- **Visual:** 3-step cycle; optional: “Like clinical reasoning: gather data → hypothesis → test → revise”  
- **Notes:** *"We never act on the first answer. We verify. That’s evidence-based practice."*

---

### Slide 8: Demo (If AI Accessible)
- **Title:** Let’s Try It  
- **Content:** Live prompt: *"Explain in one sentence what sepsis is and why early detection matters in the ICU."*  
- **Visual:** Screenshot or live AI interface  
- **Notes:** Run the prompt. Then: *"How would we verify this? What source would we check?"*

---

### Slide 9: One Risk, One Benefit
- **Title:** AI in Healthcare: Risks and Benefits  
- **Bullets:**
  - **Risks:** Misdiagnosis, bias (e.g., worse for some populations), over-reliance, privacy
  - **Benefits:** Faster triage, more consistent screening, better access in underserved areas
  - **Key idea:** Accuracy isn’t the same as fairness or safety—we have to check both
- **Visual:** Balance scale or simple pro/con table  
- **Notes:** *"As future healthcare workers, you’ll need to weigh both."*

---

### Slide 10: Guided Practice — Your Turn
- **Title:** Prompt, Explain, Verify  
- **Bullets:**
  - Write a prompt asking AI to explain one medical concept
  - Fact-check the answer against CDC, NIH, or your textbook
  - With a partner: one risk and one benefit of AI in healthcare
- **Visual:** Handout preview or worksheet icon  
- **Notes:** *"Use the handout. I’ll circulate. Be ready to share."*

---

### Slide 11: Exit Ticket
- **Title:** Before You Go  
- **Content:** Display the exit ticket question (see Section 4)  
- **Notes:** *"Submit on Google Classroom before you leave."*

---

## 4. EXIT TICKET QUESTION

**Exact wording for Google Classroom:**

---

> **Exit Ticket — Week 1**
>
> In 2–3 sentences, answer:
>
> 1. What is medical AI in plain language?
> 2. Name one place where AI is already used in healthcare (from today or your own knowledge).
> 3. Why is "Verify" especially important when using AI in medicine?
>
> Submit before you leave.

---

## 5. STUDENT HANDOUT DESCRIPTION

**Title:** Week 1 — Prompt, Explain, Verify  
**Format:** 1 page, printable or digital (Google Doc)

---

### Part 1: Prompt Writing (5 min)

**Instructions:** Write a clear prompt asking an AI to explain ONE of these:
- What vital signs and lab values are used to predict deterioration in the ICU?
- What is sepsis and what are early warning signs?
- How does AI help radiologists read X-rays?

**Template:**
> My prompt: _______________________________________________
>
> AI’s answer (or sample answer): _______________________________________________

---

### Part 2: Fact-Checking Checklist (5 min)

**Instructions:** Compare the AI’s answer to one trusted source: CDC.gov, NIH.gov, or your biology/textbook.

| Check | Yes / No | Notes |
|:------|:---------|:------|
| Does the AI’s answer match the trusted source? | | |
| Is anything missing or oversimplified? | | |
| Is anything wrong? | | |
| What would you double-check before using this in a real clinical setting? | | |

**One sentence:** What did the AI get right? What would you verify?
> _______________________________________________

---

### Part 3: One Risk, One Benefit (4 min)

**Discuss with a partner.** Then write:

- **One risk** of using AI in healthcare (e.g., misdiagnosis, bias, over-reliance, privacy):
  > _______________________________________________

- **One benefit** (e.g., access, speed, consistency):
  > _______________________________________________

**Be ready to share** one risk and one benefit with the class.

---

**Optional:** Provide a 2–3 sentence sample AI response (e.g., on sepsis or ICU prediction) so students can practice fact-checking even if live AI is blocked.

---

## SUCCESS CRITERIA

A student has succeeded in Week 1 if they can say:

> *"Medical AI finds patterns in patient data to help doctors make decisions. It’s used in [radiology / ICU monitoring / etc.], and we have to verify it so we don’t cause harm."*

---

*"First, do no harm. Verify, then act."*
